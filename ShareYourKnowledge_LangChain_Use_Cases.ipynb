{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11d788b0"
      },
      "source": [
        "# 📚 Cookbook Based on LangChain Conceptual Documentation 📖\n",
        "\n",
        "\n",
        "## ❓ What is LangChain?\n",
        "> LangChain is a framework designed for the development of applications powered by language models.\n",
        "*[Source](https://blog.langchain.dev/announcing-our-10m-seed-round-led-by-benchmark/#:~:text=LangChain%20is%20a%20framework%20for%20developing%20applications%20powered%20by%20language%20models)*\n",
        "\n",
        "**Summary**: LangChain simplifies the complexities of working with and building AI models in two key ways:\n",
        "\n",
        "\n",
        "Most examples in this notebook are inspired by [Greg Kamradt](https://www.youtube.com/watch?v=vGP4pQdCocw) who is a passionately provides contents on LangChain and its usecases. For live examples, visit the [LangChain Project Gallery](https://github.com/gkamradt/langchain-tutorials)."
      ],
      "id": "11d788b0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases of LangChain\n"
      ],
      "metadata": {
        "id": "CM2LNcdTOHuO"
      },
      "id": "CM2LNcdTOHuO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Uses**\n",
        "\n",
        "1. **Summarization** - Sum up important details from text or chat.\n",
        "2. **Q&A from Documents** - Answer questions using info from documents.\n",
        "3. **Data Extraction** - Get structured data from text or user queries.\n",
        "4. **API Interaction** - Communicate with external systems via APIs.\n",
        "5. **Tabular Data Queries** - Fetch data from tables or databases.\n",
        "\n",
        "\n",
        "**Other Roles**\n",
        "6. **Evaluation** - Check how well your app performs.\n",
        "7. **Chatbots** - Create conversational bots with memory.\n",
        "8. **Agents** - Use LLMs for smarter choices.\n",
        "9. **Understanding Code** - Decode and grasp code.\n"
      ],
      "metadata": {
        "id": "TtHYPH7rOTt_"
      },
      "id": "TtHYPH7rOTt_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PRkjTMDodvV"
      },
      "source": [
        "## Installing Libraries"
      ],
      "id": "1PRkjTMDodvV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEomBJoHiE8o",
        "outputId": "6b73ba6a-b107-4f87-c947-fd71b74b6643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.319)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.47)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.10.24)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.8.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2023.6.15)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.4.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (4.8.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.14)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.104.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.8.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.14.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.17.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.18.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install python-dotenv\n",
        "\n",
        "# installing langchain libraries\n",
        "%pip install langchain\n",
        "%pip install openai\n",
        "%pip install tiktoken\n",
        "%pip install unstructured\n",
        "\n",
        "# installing vector stores\n",
        "%pip install faiss-gpu\n",
        "%pip install chromadb"
      ],
      "id": "kEomBJoHiE8o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp-_CovQohLD"
      },
      "source": [
        "## Mounting Colab"
      ],
      "id": "xp-_CovQohLD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9l6kB0-iIHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53992a6-b51e-4f2a-ebdc-6466f7848e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "id": "Y9l6kB0-iIHq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up OpenAI Key"
      ],
      "metadata": {
        "id": "uCbuRQUvcPCc"
      },
      "id": "uCbuRQUvcPCc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9815081"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY', '')"
      ],
      "id": "e9815081"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make your dispaly a bit wider"
      ],
      "metadata": {
        "id": "jZGa8CdeoOD8"
      },
      "id": "jZGa8CdeoOD8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcd3587c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "30b068c2-ccb3-4ad2-d311-efa2798885c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:90% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run this cell if you want to make your display wider\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
      ],
      "id": "dcd3587c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbdb1dc"
      },
      "source": [
        "## 1) Summarization\n",
        "\n",
        "One of the frequently encountered applications of LangChain and Language Models (LLMs) involves text summarization. The utility of this tool extends to summarizing a wide array of content, including calls, articles, books, academic papers, legal documents, user histories, tables, and financial reports. The ability to swiftly condense information is highly valuable\n",
        "\n",
        "* **Examples** - [Summarizing B2B Sales Calls](https://www.youtube.com/watch?v=DIw4rbpI9ic)\n",
        "* **Use Cases** - Summarize Articles, Transcripts, Chat History, Slack/Discord, Customer Interactions, Medical Papers, Legal Documents, Podcasts, Tweet Threads, Code Bases, Product Reviews, Financial Documents\n",
        "\n",
        "### Summaries Of Short Text\n",
        "\n",
        "For summaries of short texts, the method is straightforward, in fact you don't need to do anything fancy other than simple prompting with instructions\n"
      ],
      "id": "1bbdb1dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Alt Text](https://drive.google.com/uc?id=1HUQ4_EPNG7DdrBUaKyUq6pbcMI4yw4gU)\n"
      ],
      "metadata": {
        "id": "tGQQ5uZQHBLC"
      },
      "id": "tGQQ5uZQHBLC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c292592"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Note, the default model is already 'text-davinci-003' but I call it out here explicitly so you know where to change it later if you want\n",
        "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
        "\n",
        "# Create our template\n",
        "template = \"\"\"\n",
        "%INSTRUCTIONS:\n",
        "Please summarize the following piece of text.\n",
        "Respond in a manner that a 8 year old would understand.\n",
        "\n",
        "%TEXT:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "# Create a LangChain prompt template that we can insert values to later\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ],
      "id": "0c292592"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f539cb53"
      },
      "source": [
        "Let's let's find a confusing text online from Wikipedia. *[Source](https://en.wikipedia.org/wiki/Network_element#:~:text=According%20to%20the%20Telecommunications%20Act,of%20such%20facility%20or%20equipment.)*"
      ],
      "id": "f539cb53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0df2cde6"
      },
      "outputs": [],
      "source": [
        "\n",
        "confusing_text = \"\"\"\n",
        "In computer networks, a network element is a manageable logical entity uniting one or more physical devices. This allows distributed devices to be managed in a unified way using one management system.\n",
        "\n",
        "According to the Telecommunications Act of 1996, the term 'network element' refers to a facility or to equipment used in the provision of a telecommunications service. This term also refers to features, functions, and capabilities that are provided by means of such facility or equipment. This includes items such as subscriber numbers, databases, signaling systems, and information that is sufficient for billing and collection. Alternatively, it's also included if it's used in the transmission, routing, or other provision of a telecommunications service.\n",
        "\"\"\""
      ],
      "id": "0df2cde6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03d31842"
      },
      "source": [
        "Let's take a look at what prompt will be sent to the LLM"
      ],
      "id": "03d31842"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "406eb8a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb886658-c526-433e-a635-6dc0894a71ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- Prompt Begin -------\n",
            "\n",
            "%INSTRUCTIONS:\n",
            "Please summarize the following piece of text.\n",
            "Respond in a manner that a 8 year old would understand.\n",
            "\n",
            "%TEXT:\n",
            "\n",
            "In computer networks, a network element is a manageable logical entity uniting one or more physical devices. This allows distributed devices to be managed in a unified way using one management system.\n",
            "\n",
            "According to the Telecommunications Act of 1996, the term 'network element' refers to a facility or to equipment used in the provision of a telecommunications service. This term also refers to features, functions, and capabilities that are provided by means of such facility or equipment. This includes items such as subscriber numbers, databases, signaling systems, and information that is sufficient for billing and collection. Alternatively, it's also included if it's used in the transmission, routing, or other provision of a telecommunications service.\n",
            "\n",
            "\n",
            "------- Prompt End -------\n"
          ]
        }
      ],
      "source": [
        "print (\"------- Prompt Begin -------\")\n",
        "\n",
        "\n",
        "# create the actual prompt\n",
        "final_prompt = prompt.format(text=confusing_text)\n",
        "\n",
        "# how does the final prompt look like\n",
        "print(final_prompt)\n",
        "\n",
        "print (\"------- Prompt End -------\")"
      ],
      "id": "406eb8a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a95e53d9"
      },
      "source": [
        "Finally let's pass it through the LLM"
      ],
      "id": "a95e53d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc7e4b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff62baec-b0fb-4875-9502-dba5d82ae1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Network elements are pieces of equipment or facilities that help computers talk to each other. They help computers send messages and information to each other, like a telephone does. They also help computers keep track of who is sending what messages and how much it costs.\n"
          ]
        }
      ],
      "source": [
        "output = llm(final_prompt)\n",
        "print (output)"
      ],
      "id": "bc7e4b42"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "751c6359"
      },
      "source": [
        "This approach functions effectively; however, when dealing with extended text, it can become cumbersome to handle and may encounter limitations related to the number of tokens.\n",
        "\n",
        "'text-davinci-003' model has token constraint of 4097 tokens which include prompt and response combined.\n",
        "\n",
        "\n",
        "Fortunately, LangChain provides built-in support for various summarization methods through their [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html) feature.\n",
        "\n",
        "### Summaries Of Longer Text\n",
        "\n",
        "*Note: This method will also work for short text too*"
      ],
      "id": "751c6359"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3441484b"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n"
      ],
      "id": "3441484b"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from a url\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "urls = [\n",
        "   \"https://www.reailize.com/post/the-power-of-ticket-automation-accelerating-ticket-resolution-time-unlocking-opex-benefits\",\n",
        "]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "docs = loader.load()\n",
        "# print(docs[0].page_content[:500])\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "u4sYsBVfPB2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927ba97e-1bd1-453c-f0a9-d77e27c9ca62"
      },
      "id": "u4sYsBVfPB2-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Patryk Debicki\\n\\nThe Power of Ticket Automation - \\u200bAccelerating Ticket Resolution Time & Unlocking OPEX Benefits\\n\\nIntroduction\\n\\nEfficient ticket handling becomes paramount as the telecommunications industry embraces new advancements like 5G and technology stack disaggregation. Manual processes and lengthy queueing times can no longer keep up with these cutting-edge networks’ increasing complexity and demands. However, by harnessing the potential of advanced AI/ML algorithms and ticket automation, network operators can experience a transformative shift in their operations. This blog explains the ticket creation and resolution process and delves into the benefits of using ML algorithms and automation at different operational stages to address challenges amplified by the increased complexity of modern networks.\\n\\nProcess of Ticket Creation and Resolution\\n\\n1.1 Ticket Creation: Origins and Varieties\\n\\nTicket origins can vary widely from sources such as human submissions to address customer complaints, equipment faults and alarms or to resolve network performance issues. Additionally, tickets can be automatically generated through alarm automation or third-party systems. This diverse range of ticket origins reflects the multifaceted nature of the issues that the network operations must handle.\\n\\nAutomated ticket creation through alarming systems often marks the initial step in the automation journey of a Network Operations Centre (NOC). Many fault management systems already have capabilities to automate ticket creation based on a received alarm type and a set of business rules. However, next steps that would include automated triaging, root cause determination and remediation automation require more sophisticated AI/ML approach.\\n\\n1.2 Queue Allocation and Ticket Troubleshooting\\n\\nOnce a ticket is created, it is allocated to a specific queue manually by a human operator or automatically by a system. However, accurate queue allocation can be challenging, leading to inefficiencies. Tickets may spend unnecessary time in a queue, only to be later reassigned to another team or operator, thereby extending the waiting time. Accurate and efficient queue allocation is crucial to minimize delays and promptly ensure tickets reach the appropriate teams or individuals.\\n\\nOnce a ticket is correctly allocated to a team responsible for its resolution, it must reach the front of the queue before the troubleshooting process starts. This stage involves analysing the issue, identifying its root cause, and devising an appropriate resolution action. The duration of troubleshooting can vary significantly, ranging from a few minutes for simple problems to several days for complex issues. The expertise and efficiency of the support team play a vital role in expediting this stage.\\n\\n1.3 Data Science Approach for Ticket Allocation and Resolution\\n\\nAddressing the challenges of ticket allocation and troubleshooting requires a comprehensive method that can handle both tasks simultaneously. In this context, a data science approach, that leverages information retrieval techniques and a Large Language Model (LLM), effectively matches and analyses tickets.\\n\\nThe foundation of this method lies in the information retrieval task. By utilizing a database of historical tickets and the description of a new incoming ticket, the system endeavours to find the most relevant past ticket that closely matches the new one. This is achieved through the semantic similarity of the text content embedded in the tickets. The LLM plays a crucial role in understanding and comparing the ticket content, leading to the generation of a similarity score for each potential match. The top matched entries, along with their corresponding similarity scores, are then provided as the output.\\n\\nOnce the tickets are successfully matched based on their semantic similarity, the next step is to engage a recommender system that takes into consideration various ticket attributes. These attributes include the ticket origin; such as customer care, service request, change management, or alarm-triggered, as well as network element and user associations, ticket severity, and others. The recommender system effectively processes this information and generates recommendations for the team most likely to resolve the ticket and the best resolution approach.\\n\\nAnother benefit of using the LLM is the question answering capability. A user can ask specific questions to the system to get additional insights. For example, questions like “will restarting the software resolve the problem of high CPU usage?” or “can you create a report with this information?”.\\n\\nAn important aspect of the process is model evaluation and the assessment of system sensitivity and specify (false positives, false negatives). This is being continuously assessed and if the model\\'s performance is unsatisfactory, iterations can be made by refining the pre-processing steps, adjusting the model architecture, or augmenting the dataset to improve predictions.\\n\\nAnother dimension of the problem that we account for is the model life cycle. To sustain, the algorithm needs to get access to updated databases. This can be addressed by leveraging tools such as vector databases. Those tools represent information as a vector of numbers and an algorithm manages to retrieve information in a timely manner.\\n\\nBy combining the power of information retrieval with a Large Language Model and a feature-rich recommender system, this data science approach streamlines the ticket allocation and resolution process. It enables efficient ticket handling by identifying past cases with similar characteristics and suggesting the most suitable team and resolution strategy for the new tickets. This approach ultimately enhances ticket management efficiency and ensures quicker and more effective problem resolution.\\n\\n1.4 Remediation Automation\\n\\nAs observed earlier, the Root Cause Analysis (RCA) process leads to identifying the most appropriate resolution action for a given issue. This action could involve escalating the ticket to a higher level of support for further investigation or automating specific tasks, such as software restarts, system re-provisioning, or parameter changes.\\n\\nAutomating such resolution actions is feasible when they are repeatable and well-documented. Typically, support teams maintain a library of Methods of Procedure (MOP) that can be automated using a hyper-automation framework encompassing various access methods like Robotic Process Automation (RPA), Command-Line Interface (CLI), and Application Programming Interfaces (API).\\n\\nAn output-based decisioning engine further enhances the framework\\'s capabilities. It is a component of an automated system that makes decisions or takes actions based on the output or results of previous steps or processes. It is designed to evaluate the outcomes or predictions produced by various algorithms, models, or modules within the system and then make informed decisions or initiate further actions accordingly.\\n\\nAs part of the remediation automation process configuration, the support team creates templates for specific MOPs and integrates it into the automation framework. Consequently, an API is exposed for the corresponding MOP. When the system provides a remediation recommendation that matches an already exposed API, it can be automatically triggered, streamlining the resolution process.\\n\\nBy automating repeatable tasks through a well-structured hyper-automation framework, the support team can improve efficiency, reduce manual intervention, and accelerate the resolution of issues. The combination of various access methods and the output-based decisioning engine ensures flexibility and adaptability to diverse scenarios, contributing to more efficient and reliable support operations.\\n\\n1.5 Ticket Closure: Validating Outcomes and Completion\\n\\nOnce the resolution action is executed, its outcome is thoroughly validated to ensure the issue has been successfully addressed. This verification process confirms that the desired result has been achieved and the problem has been resolved to the satisfaction of the customer or network performance requirements. With a validated outcome, the ticket can be closed, marking the completion of the resolution process. In the NOC automation transformation, the validation process is just another step within the output-based decisioning engine of the automated ticket resolution.\\n\\nBusiness Case for NOC Transformation\\n\\nOnce the ML algorithms allocate the correct queue and predict the ticket resolution type, the most evident advantage of ticket automation is its ability to reduce the Mean Time to Repair (MTTR) drastically. By leveraging automation, what might have taken hours can now be accomplished within minutes, allowing support teams to address issues promptly. Moreover, even if the actual task execution time was relatively short, tickets often spent a substantial duration in the queue awaiting manual processing. It is common for queueing time to account for most of the MTTR. For instance, if an employee spends 10 minutes executing a software restart, but the ticket spends 90 minutes in the queue, the overall MTTR is 100 minutes. In most cases, software restarts are ticket types where ML copes very well with the prediction precision; hence, they can be fully automated. In this example, a reduction from 100 to just 1 minute by automating the ticket workflow is perfectly achievable. This represents a staggering 99% reduction in the overall resolution time. On the overall operational scale, 40-50% of MTTR reduction is feasible.\\n\\nTicket automation goes beyond mere task execution. By automating routine and repetitive ticket-handling processes, valuable time is freed up for support personnel. With reduced queueing time for simpler tickets, employees can focus their expertise on more complex issues, accelerating their resolution. This ripple effect within the support team creates a positive feedback loop that further drives the overall MTTR down. Partial automation can already yield significant improvements, but the MTTR reduction can reach its maximum potential as it becomes more pervasive.\\n\\nThere are significant business benefits of the MTTR reduction and ticket automation;  below are a few examples:\\n\\nImproved Customer Satisfaction: Customer satisfaction levels rise when issues are resolved quickly and efficiently. By reducing the MTTR, businesses can address customer concerns promptly, leading to happier and more loyal customers. Satisfied customers are likelier to remain loyal, refer others to the business, and contribute to a positive brand reputation. This particularly applies to customer care-related tickets where each 1% improvement in First Call Resolution (FCR) increases transactional NPS by 1.4 points for an average call centre￼\\n\\nMinimized Downtime: Downtime can be costly for businesses, leading to lost productivity, revenue, and customer trust. By reducing MTTR, organizations can minimize the duration of service disruptions and outages. This translates to less downtime for customers, enabling them to continue their operations smoothly and reducing the negative impact on business operations. This leads to improved NPS and reduced CHURN.\\n\\nIncreased Productivity: MTTR reduction allows employees to spend less time troubleshooting and resolving issues. This frees up their valuable time, enabling them to focus on more strategic tasks, projects, and customer-facing activities. Improved productivity can lead to better efficiency, innovation, and overall business growth.\\n\\nUnlocking OPEX Benefits: While the primary motivation for ticket automation is often to enhance service delivery and reduce MTTR, the benefits extend beyond efficiency gains. As the MTTR approaches its minimum achievable level, organizations can leverage the automation infrastructure to drive substantial OPEX reduction. With reduced manual intervention, personnel resources can be optimized, leading to potential cost savings. This OPEX reduction becomes particularly relevant in the current economic climate, where organizations increasingly focus on optimizing their operations.\\n\\nImagine a hypothetical (but realistic) business case for ticket automation. Begin by assuming that an operator is having around 120,000 tickets per year, 1/3 of which are already automated to some extent and 1/3 are not feasible for automation for different reasons (e.g. issues requiring local presence). This leaves 40k tickets feasible for automation (see Figure 1). The average ticket resolution time for this class of tickets is 1.5 days. The staff engagement level for these tickets is around 10% (i.e., 90% of time ticket spends in queues and idle, waiting to be resolved). This gives 6,000 calendar days or 18,000 man-days needed to resolve all these tickets. Multiplying this by the cost ($500) of a Full Time Employee (FTE) and taking a conservative figure of the percentage of the time reduction associated with the automation (30-50%), it arrives to the savings range of $2.7 to $4.5 million. The saving figure doesn’t include additional benefits of MTTR reduction like improvement in the customer satisfaction.\\n\\nConclusion\\n\\nIn conclusion, the telecommunications industry is facing a pressing need for efficient ticket handling as networks become more complex with the advent of technologies like 5G SA and stack disaggregation. By harnessing the power of AI/ML algorithms and ticket automation, network operators can revolutionize their operations, leading to reduced MTTR, improved customer satisfaction, and increased productivity.\\n\\nThe data science approach, utilizing information retrieval and Large Language Models, streamlines ticket allocation and resolution by providing relevant historical matches and valuable recommendations for resolution teams. Integrating an output-based decisioning engine further enhances the system\\'s adaptability and decision-making capabilities.\\n\\nTicket automation yields a plethora of benefits, from optimizing personnel resources and driving substantial OPEX reduction to minimizing downtime and unlocking improvements in customer satisfaction.\\n\\nLooking ahead, Reailize envisions a future where 100% of tickets can be automated, leading to a truly \"dark NOC\" - a seamlessly automated Network Operations Centre where lights can be switched off, as AI-driven processes take the lead. With this vision in mind, Reailize sets forth on a transformative journey towards streamlined and highly efficient network operations in the telecommunications industry.\\n\\nAre you ready to embrace the potential of AI and automation to achieve a \"dark NOC\" future?', metadata={'source': 'https://www.reailize.com/post/the-power-of-ticket-automation-accelerating-ticket-resolution-time-unlocking-opex-benefits'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## convert to text\n",
        "text=\"\"\n",
        "\n",
        "for page in docs:\n",
        "    text+=page.page_content\n",
        "text= text.replace('\\t', ' ')\n",
        "\n",
        "print(len(text))\n"
      ],
      "metadata": {
        "id": "7berbKCGTYv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1052913-8f15-4b2f-c1e8-883714664314"
      },
      "id": "7berbKCGTYv6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"There are {num_tokens} tokens in your file\")"
      ],
      "metadata": {
        "id": "6Z9bjaA7Sxed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb4fbc7-bc52-4181-cd90-87366110f0a4"
      },
      "id": "6Z9bjaA7Sxed",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2720 tokens in your file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b489d2a2"
      },
      "source": [
        "Then let's check how many tokens are in this document. [get_num_tokens](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI.get_num_tokens) is a nice method for this."
      ],
      "id": "b489d2a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bf8eda6"
      },
      "source": [
        "While you could likely stuff this text in your prompt, let's act like it's too big and needs another method.\n",
        "\n",
        "First we'll need to split it up. This process is called 'chunking' or 'splitting' your text into smaller pieces. I like the [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html) because it's easy to control but there are a [bunch](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) you can try\n",
        "\n"
      ],
      "id": "5bf8eda6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25dd80dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea4c883-99fd-48a0-9321-d22fdeb305d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You now have 4 docs intead of 1 piece of text\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=4000, chunk_overlap=350)\n",
        "docs = text_splitter.create_documents([text])\n",
        "\n",
        "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")"
      ],
      "id": "25dd80dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e7547a3"
      },
      "source": [
        "Next we need to load up a chain which will make successive calls to the LLM for us. Want to see the prompt being used in the chain below? Check out the [LangChain documentation](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py)\n",
        "\n",
        "For information on the difference between chain types, check out this video on [token limit workarounds](https://youtu.be/f9_BWhCI4Zo)\n",
        "\n",
        "*Note: You could also get fancy and make the first 4 calls of the map_reduce run in parallel too*"
      ],
      "id": "3e7547a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28ddd9c0"
      },
      "outputs": [],
      "source": [
        "# Get your chain ready to use\n",
        "chain = load_summarize_chain(llm=llm, chain_type='stuff', verbose=True) # verbose=True optional to see what is getting sent to the LLM"
      ],
      "id": "28ddd9c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be0b2d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5520d8cf-4027-4ee9-e98d-bac5d9a4e554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 9.3 µs\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"Patryk Debicki\n",
            "\n",
            "The Power of Ticket Automation - ​Accelerating Ticket Resolution Time & Unlocking OPEX Benefits\n",
            "\n",
            "Introduction\n",
            "\n",
            "Efficient ticket handling becomes paramount as the telecommunications industry embraces new advancements like 5G and technology stack disaggregation. Manual processes and lengthy queueing times can no longer keep up with these cutting-edge networks’ increasing complexity and demands. However, by harnessing the potential of advanced AI/ML algorithms and ticket automation, network operators can experience a transformative shift in their operations. This blog explains the ticket creation and resolution process and delves into the benefits of using ML algorithms and automation at different operational stages to address challenges amplified by the increased complexity of modern networks.\n",
            "\n",
            "Process of Ticket Creation and Resolution\n",
            "\n",
            "1.1 Ticket Creation: Origins and Varieties\n",
            "\n",
            "Ticket origins can vary widely from sources such as human submissions to address customer complaints, equipment faults and alarms or to resolve network performance issues. Additionally, tickets can be automatically generated through alarm automation or third-party systems. This diverse range of ticket origins reflects the multifaceted nature of the issues that the network operations must handle.\n",
            "\n",
            "Automated ticket creation through alarming systems often marks the initial step in the automation journey of a Network Operations Centre (NOC). Many fault management systems already have capabilities to automate ticket creation based on a received alarm type and a set of business rules. However, next steps that would include automated triaging, root cause determination and remediation automation require more sophisticated AI/ML approach.\n",
            "\n",
            "1.2 Queue Allocation and Ticket Troubleshooting\n",
            "\n",
            "Once a ticket is created, it is allocated to a specific queue manually by a human operator or automatically by a system. However, accurate queue allocation can be challenging, leading to inefficiencies. Tickets may spend unnecessary time in a queue, only to be later reassigned to another team or operator, thereby extending the waiting time. Accurate and efficient queue allocation is crucial to minimize delays and promptly ensure tickets reach the appropriate teams or individuals.\n",
            "\n",
            "Once a ticket is correctly allocated to a team responsible for its resolution, it must reach the front of the queue before the troubleshooting process starts. This stage involves analysing the issue, identifying its root cause, and devising an appropriate resolution action. The duration of troubleshooting can vary significantly, ranging from a few minutes for simple problems to several days for complex issues. The expertise and efficiency of the support team play a vital role in expediting this stage.\n",
            "\n",
            "1.3 Data Science Approach for Ticket Allocation and Resolution\n",
            "\n",
            "Addressing the challenges of ticket allocation and troubleshooting requires a comprehensive method that can handle both tasks simultaneously. In this context, a data science approach, that leverages information retrieval techniques and a Large Language Model (LLM), effectively matches and analyses tickets.\n",
            "\n",
            "The foundation of this method lies in the information retrieval task. By utilizing a database of historical tickets and the description of a new incoming ticket, the system endeavours to find the most relevant past ticket that closely matches the new one. This is achieved through the semantic similarity of the text content embedded in the tickets. The LLM plays a crucial role in understanding and comparing the ticket content, leading to the generation of a similarity score for each potential match. The top matched entries, along with their corresponding similarity scores, are then provided as the output.\n",
            "\n",
            "Once the tickets are successfully matched based on their semantic similarity, the next step is to engage a recommender system that takes into consideration various ticket attributes. These attributes include the ticket origin; such as customer care, service request, change management, or alarm-triggered, as well as network element and user associations, ticket severity, and others. The recommender system effectively processes this information and generates recommendations for the team most likely to resolve the ticket and the best resolution approach.\n",
            "\n",
            "Another benefit of using the LLM is the question answering capability. A user can ask specific questions to the system to get additional insights. For example, questions like “will restarting the software resolve the problem of high CPU usage?” or “can you create a report with this information?”.\n",
            "\n",
            "An important aspect of the process is model evaluation and the assessment of system sensitivity and specify (false positives, false negatives). This is being continuously assessed and if the model's performance is unsatisfactory, iterations can be made by refining the pre-processing steps, adjusting the model architecture, or augmenting the dataset to improve predictions.\n",
            "\n",
            "Another dimension of the problem that we account for is the model life cycle. To sustain, the algorithm needs to get access to updated databases. This can be addressed by leveraging tools such as vector databases. Those tools represent information as a vector of numbers and an algorithm manages to retrieve information in a timely manner.\n",
            "\n",
            "By combining the power of information retrieval with a Large Language Model and a feature-rich recommender system, this data science approach streamlines the ticket allocation and resolution process. It enables efficient ticket handling by identifying past cases with similar characteristics and suggesting the most suitable team and resolution strategy for the new tickets. This approach ultimately enhances ticket management efficiency and ensures quicker and more effective problem resolution.\n",
            "\n",
            "1.4 Remediation Automation\n",
            "\n",
            "As observed earlier, the Root Cause Analysis (RCA) process leads to identifying the most appropriate resolution action for a given issue. This action could involve escalating the ticket to a higher level of support for further investigation or automating specific tasks, such as software restarts, system re-provisioning, or parameter changes.\n",
            "\n",
            "Automating such resolution actions is feasible when they are repeatable and well-documented. Typically, support teams maintain a library of Methods of Procedure (MOP) that can be automated using a hyper-automation framework encompassing various access methods like Robotic Process Automation (RPA), Command-Line Interface (CLI), and Application Programming Interfaces (API).\n",
            "\n",
            "An output-based decisioning engine further enhances the framework's capabilities. It is a component of an automated system that makes decisions or takes actions based on the output or results of previous steps or processes. It is designed to evaluate the outcomes or predictions produced by various algorithms, models, or modules within the system and then make informed decisions or initiate further actions accordingly.\n",
            "\n",
            "As part of the remediation automation process configuration, the support team creates templates for specific MOPs and integrates it into the automation framework. Consequently, an API is exposed for the corresponding MOP. When the system provides a remediation recommendation that matches an already exposed API, it can be automatically triggered, streamlining the resolution process.\n",
            "\n",
            "By automating repeatable tasks through a well-structured hyper-automation framework, the support team can improve efficiency, reduce manual intervention, and accelerate the resolution of issues. The combination of various access methods and the output-based decisioning engine ensures flexibility and adaptability to diverse scenarios, contributing to more efficient and reliable support operations.\n",
            "\n",
            "1.5 Ticket Closure: Validating Outcomes and Completion\n",
            "\n",
            "Once the resolution action is executed, its outcome is thoroughly validated to ensure the issue has been successfully addressed. This verification process confirms that the desired result has been achieved and the problem has been resolved to the satisfaction of the customer or network performance requirements. With a validated outcome, the ticket can be closed, marking the completion of the resolution process. In the NOC automation transformation, the validation process is just another step within the output-based decisioning engine of the automated ticket resolution.\n",
            "\n",
            "Business Case for NOC Transformation\n",
            "\n",
            "Once the ML algorithms allocate the correct queue and predict the ticket resolution type, the most evident advantage of ticket automation is its ability to reduce the Mean Time to Repair (MTTR) drastically. By leveraging automation, what might have taken hours can now be accomplished within minutes, allowing support teams to address issues promptly. Moreover, even if the actual task execution time was relatively short, tickets often spent a substantial duration in the queue awaiting manual processing. It is common for queueing time to account for most of the MTTR. For instance, if an employee spends 10 minutes executing a software restart, but the ticket spends 90 minutes in the queue, the overall MTTR is 100 minutes. In most cases, software restarts are ticket types where ML copes very well with the prediction precision; hence, they can be fully automated. In this example, a reduction from 100 to just 1 minute by automating the ticket workflow is perfectly achievable. This represents a staggering 99% reduction in the overall resolution time. On the overall operational scale, 40-50% of MTTR reduction is feasible.\n",
            "\n",
            "Ticket automation goes beyond mere task execution. By automating routine and repetitive ticket-handling processes, valuable time is freed up for support personnel. With reduced queueing time for simpler tickets, employees can focus their expertise on more complex issues, accelerating their resolution. This ripple effect within the support team creates a positive feedback loop that further drives the overall MTTR down. Partial automation can already yield significant improvements, but the MTTR reduction can reach its maximum potential as it becomes more pervasive.\n",
            "\n",
            "There are significant business benefits of the MTTR reduction and ticket automation;  below are a few examples:\n",
            "\n",
            "Improved Customer Satisfaction: Customer satisfaction levels rise when issues are resolved quickly and efficiently. By reducing the MTTR, businesses can address customer concerns promptly, leading to happier and more loyal customers. Satisfied customers are likelier to remain loyal, refer others to the business, and contribute to a positive brand reputation. This particularly applies to customer care-related tickets where each 1% improvement in First Call Resolution (FCR) increases transactional NPS by 1.4 points for an average call centre￼\n",
            "\n",
            "Minimized Downtime: Downtime can be costly for businesses, leading to lost productivity, revenue, and customer trust. By reducing MTTR, organizations can minimize the duration of service disruptions and outages. This translates to less downtime for customers, enabling them to continue their operations smoothly and reducing the negative impact on business operations. This leads to improved NPS and reduced CHURN.\n",
            "\n",
            "Increased Productivity: MTTR reduction allows employees to spend less time troubleshooting and resolving issues. This frees up their valuable time, enabling them to focus on more strategic tasks, projects, and customer-facing activities. Improved productivity can lead to better efficiency, innovation, and overall business growth.\n",
            "\n",
            "Unlocking OPEX Benefits: While the primary motivation for ticket automation is often to enhance service delivery and reduce MTTR, the benefits extend beyond efficiency gains. As the MTTR approaches its minimum achievable level, organizations can leverage the automation infrastructure to drive substantial OPEX reduction. With reduced manual intervention, personnel resources can be optimized, leading to potential cost savings. This OPEX reduction becomes particularly relevant in the current economic climate, where organizations increasingly focus on optimizing their operations.\n",
            "\n",
            "Imagine a hypothetical (but realistic) business case for ticket automation. Begin by assuming that an operator is having around 120,000 tickets per year, 1/3 of which are already automated to some extent and 1/3 are not feasible for automation for different reasons (e.g. issues requiring local presence). This leaves 40k tickets feasible for automation (see Figure 1). The average ticket resolution time for this class of tickets is 1.5 days. The staff engagement level for these tickets is around 10% (i.e., 90% of time ticket spends in queues and idle, waiting to be resolved). This gives 6,000 calendar days or 18,000 man-days needed to resolve all these tickets. Multiplying this by the cost ($500) of a Full Time Employee (FTE) and taking a conservative figure of the percentage of the time reduction associated with the automation (30-50%), it arrives to the savings range of $2.7 to $4.5 million. The saving figure doesn’t include additional benefits of MTTR reduction like improvement in the customer satisfaction.\n",
            "\n",
            "Conclusion\n",
            "\n",
            "In conclusion, the telecommunications industry is facing a pressing need for efficient ticket handling as networks become more complex with the advent of technologies like 5G SA and stack disaggregation. By harnessing the power of AI/ML algorithms and ticket automation, network operators can revolutionize their operations, leading to reduced MTTR, improved customer satisfaction, and increased productivity.\n",
            "\n",
            "The data science approach, utilizing information retrieval and Large Language Models, streamlines ticket allocation and resolution by providing relevant historical matches and valuable recommendations for resolution teams. Integrating an output-based decisioning engine further enhances the system's adaptability and decision-making capabilities.\n",
            "\n",
            "Ticket automation yields a plethora of benefits, from optimizing personnel resources and driving substantial OPEX reduction to minimizing downtime and unlocking improvements in customer satisfaction.\n",
            "\n",
            "Looking ahead, Reailize envisions a future where 100% of tickets can be automated, leading to a truly \"dark NOC\" - a seamlessly automated Network Operations Centre where lights can be switched off, as AI-driven processes take the lead. With this vision in mind, Reailize sets forth on a transformative journey towards streamlined and highly efficient network operations in the telecommunications industry.\n",
            "\n",
            "Are you ready to embrace the potential of AI and automation to achieve a \"dark NOC\" future?\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "This blog explains the ticket creation and resolution process and delves into the benefits of using ML algorithms and automation at different operational stages to address challenges amplified by the increased complexity of modern networks. By combining the power of information retrieval with a Large Language Model and a feature-rich recommender system, this data science approach streamlines the ticket allocation and resolution process. Ticket automation yields a plethora of benefits, from optimizing personnel resources and driving substantial OPEX reduction to minimizing downtime and unlocking improvements in customer satisfaction. Reailize envisions a future where 100% of tickets can be automated, leading to a \"dark NOC\" - a seamlessly automated Network Operations Centre.\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "# Use it. This will run through the 4 documents, summarize the chunks, then get a summary of the summary.\n",
        "output = chain.run(docs)\n",
        "print (output)\n",
        "\n"
      ],
      "id": "be0b2d04"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2d664fc"
      },
      "source": [
        "## 2) Question & Answering Using Documents As Context"
      ],
      "id": "a2d664fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad87c72b"
      },
      "source": [
        "*[LangChain Question & Answer Docs](https://python.langchain.com/en/latest/use_cases/question_answering.html)*\n",
        "\n",
        "\n",
        "Extremely common example of chatting with your document.\n",
        "\n",
        "In order to use LLMs for question and answer we must:\n",
        "\n",
        "1. Pass the LLM relevant context it needs to answer a question\n",
        "2. Pass it our question that we want answered\n",
        "\n",
        "Simplified, this process looks like this \"llm(your context + your question) = your answer\"\n",
        "\n",
        "* **Use Cases** - Chat your documents, ask questions to academic papers, create study guides, reference medical information, [PDF.ai](https://pdf.ai/)"
      ],
      "id": "ad87c72b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "685e15f3"
      },
      "source": [
        "### Simple Q&A Example\n",
        "\n",
        "Here let's review the convention of `llm(your context + your question) = your answer`"
      ],
      "id": "685e15f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ebd8451"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY', '')\n",
        "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n"
      ],
      "id": "9ebd8451"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4795187"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "Nancy is 20 years old\n",
        "Shen is 43 years old\n",
        "Ahmed is 67 years old\n",
        "\"\"\"\n",
        "\n",
        "question = \"Who is under 40 years old?\""
      ],
      "id": "b4795187"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2184b11b"
      },
      "source": [
        "Then combine them."
      ],
      "id": "2184b11b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c53650d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0c9827-59d5-4b2f-e13c-9d92355bec18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nancy is under 40 years old.\n"
          ]
        }
      ],
      "source": [
        "output = llm(context + question)\n",
        "\n",
        "# I strip the text to remove the leading and trailing whitespace\n",
        "print (output.strip())"
      ],
      "id": "0c53650d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Augmented Generation (RAG)\n",
        "\n",
        " Using Embeddings, which is going to be a vector representation of different chunks and different documentation. -> Simply 😇 can be explained as your document is going to be converted to a big long vector that is going to be much easier to apply similarity search on!\n",
        "\n",
        " In the YouTube tutorial of Greg he often refers to this process as \"The VectorStore Dance\". It's the process of splitting your text, embedding the chunks, putting the embeddings in a DB, and then querying them.\n",
        "\n",
        "1. [Load](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
        "2. [Split/Transform](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
        "3. [Store/Embed](https://python.langchain.com/docs/modules/data_connection/text_embedding/)\n",
        "4. [Retrieve](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
        "\n",
        "The goal is to select relevant chunks of our long text, but which chunks do we pull? The most popular method is to pull *similar* texts based off comparing vector embeddings.\n"
      ],
      "metadata": {
        "id": "5LGYjSL3FhQg"
      },
      "id": "5LGYjSL3FhQg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Q & A](https://drive.google.com/uc?id=1iAqI1Rl2QDlsLPtkxfop0fqFC3PoMW98)\n"
      ],
      "metadata": {
        "id": "3DvtRa3hIbvP"
      },
      "id": "3DvtRa3hIbvP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be503d53"
      },
      "source": [
        "If you wanted to do more you would hook this up to a cloud vector database, use a tool like metal and start managing your documents, with external data sources"
      ],
      "id": "be503d53"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install faiss"
      ],
      "metadata": {
        "id": "F8d2uzXEeqhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8f57dd-7c39-4932-c066-31649689f39d"
      },
      "id": "F8d2uzXEeqhD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "# Read the information from a website\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# The vectorstore we'll be using\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# The LangChain component we'll use to get the documents\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "# The embedding engine that will convert our text to vectors\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Load your model\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "LqSBymZd5D4j"
      },
      "id": "LqSBymZd5D4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used is from [Wikipedia](https://en.wikipedia.org/wiki/3GPP)"
      ],
      "metadata": {
        "id": "Wckx8FzAUY8F"
      },
      "id": "Wckx8FzAUY8F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load from wikipedia\n",
        "\n",
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/3GPP\")\n",
        "doc = loader.load()\n",
        "\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x97gPVwQVBx",
        "outputId": "044c702e-1853-4b76-eee0-386799609227"
      },
      "id": "0x97gPVwQVBx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 document\n",
            "You have 21437 characters in that document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![RAG](https://drive.google.com/uc?id=1040sgPM0UHB7-Qzeutpq0JVE8JPl0ypM)\n"
      ],
      "metadata": {
        "id": "kCA28nE9gO1H"
      },
      "id": "kCA28nE9gO1H"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Split documents\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 10)\n",
        "docs = text_splitter.split_documents(loader.load())"
      ],
      "metadata": {
        "id": "p4EAjV1SQbAM"
      },
      "id": "p4EAjV1SQbAM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of characters so we can see the average later\n",
        "num_total_characters = sum([len(x.page_content) for x in docs])\n",
        "\n",
        "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIY03kBPRByL",
        "outputId": "34badadb-5ccd-4e04-94ac-02ac7be7bc1e"
      },
      "id": "QIY03kBPRByL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 57 documents that have an average of 375 characters (smaller pieces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Embeding\n",
        "\n",
        "# Get your embeddings engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
        "docsearch = FAISS.from_documents(docs, embeddings)\n"
      ],
      "metadata": {
        "id": "u6aY9yAyQe3x"
      },
      "id": "u6aY9yAyQe3x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Retrive\n",
        "\n",
        "# Retrival engine\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
        "\n",
        "\n",
        "query = \"Who are the organizational partners for the 3GPP?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "j4IEKAhRevKV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cbfb4ad9-4092-4741-c3df-bee3a4e73148"
      },
      "id": "j4IEKAhRevKV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The seven 3GPP Organizational Partners are the Association of Radio Industries and Businesses (ARIB) from Japan, the Alliance for Telecommunications Industry Solutions (ATIS) from the USA, and the China Communications Standards Association (CCSA) from China.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3d04dc9"
      },
      "source": [
        "## 3) Extraction\n",
        "*[LangChain Extraction Docs](https://python.langchain.com/en/latest/use_cases/extraction.html)*\n",
        "\n",
        "(this part is derived from the tutorial by [Greg Kamradt](https://www.youtube.com/watch?v=vGP4pQdCocw))\n",
        "\n",
        "Extraction is the process of parsing data from a piece of text. This is commonly used with output parsing in order to *structure* our data.\n",
        "\n",
        "\n",
        "* **Examples** - [OpeningAttributes](https://twitter.com/GregKamradt/status/1646500373837008897)\n",
        "\n",
        "* **Use Cases:** Extract a structured row from a sentence to insert into a database, extract multiple rows from a long document to insert into a database, extracting parameters from a user query to make an API call\n",
        "\n",
        "A popular library for extraction is [Kor](https://eyurtsev.github.io/kor/)."
      ],
      "id": "d3d04dc9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Extraction](https://drive.google.com/uc?id=15cuEtEnc4KUfxaFtW-BttnVa9TLR8sSc)\n"
      ],
      "metadata": {
        "id": "boUtXxK1I5LV"
      },
      "id": "boUtXxK1I5LV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "904d43c0"
      },
      "outputs": [],
      "source": [
        "# To help construct our Chat Messages\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# To parse outputs and get structured data back\n",
        "# StructuredOutputParser, ResponseSchema are being used to get the data out\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)"
      ],
      "id": "904d43c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6923ca8b"
      },
      "source": [
        "### Vanilla Extraction\n",
        "\n",
        "Let's start off with an easy example. Here I simply supply a prompt with instructions with the type of output I want."
      ],
      "id": "6923ca8b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab1cce97"
      },
      "outputs": [],
      "source": [
        "instructions = \"\"\"\n",
        "You will be given a sentence with feeling names, extract those feeling names and assign an emoji to them\n",
        "Return the feeling  name and emojis in a python dictionary\n",
        "\"\"\"\n",
        "\n",
        "feeling_names = \"\"\"\n",
        "Happiness, Shock, Sadness, Delighfulness, Calm, Chill\n",
        "\"\"\""
      ],
      "id": "ab1cce97"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38f16ea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32afbf44-316f-4a1f-d036-4da1e745fce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Happiness\": \"😄\",\n",
            "  \"Shock\": \"😱\",\n",
            "  \"Sadness\": \"😢\",\n",
            "  \"Delightfulness\": \"😊\",\n",
            "  \"Calm\": \"😌\",\n",
            "  \"Chill\": \"😎\"\n",
            "}\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "# Make your prompt which combines the instructions w/ the feeling names\n",
        "prompt = (instructions + feeling_names)\n",
        "\n",
        "# Call the LLM\n",
        "output = chat_model([HumanMessage(content=prompt)])\n",
        "\n",
        "print (output.content)\n",
        "print (type(output.content))"
      ],
      "id": "38f16ea4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39d6cff3"
      },
      "source": [
        "Let's turn this into a proper python dictionary"
      ],
      "id": "39d6cff3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "314286b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab53e67-ab48-41ca-ec80-bd974a64ce2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Happiness': '😄', 'Shock': '😱', 'Sadness': '😢', 'Delightfulness': '😊', 'Calm': '😌', 'Chill': '😎'}\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "output_dict = eval(output.content)\n",
        "\n",
        "print (output_dict)\n",
        "print (type(output_dict))"
      ],
      "id": "314286b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3909eb29"
      },
      "source": [
        "While this worked this time, it's not a long term reliable method for more advanced use cases and we may need to use **prompt engineering**"
      ],
      "id": "3909eb29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6a0a90d"
      },
      "source": [
        "### Using LangChain's Response Schema\n",
        "\n",
        "LangChain's response schema will does two things for us:\n",
        "\n",
        "1. Autogenerate the a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
        "\n",
        "2. Read the output from the LLM and turn it into a proper python object for me\n",
        "\n",
        "Here I define the schema I want. I'm going to pull out the song and artist that a user wants to play from a pseudo chat message."
      ],
      "id": "b6a0a90d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc2ba0be"
      },
      "outputs": [],
      "source": [
        "# The schema I want out\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
        "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
        "]\n",
        "\n",
        "# The parser that will look for the LLM output in my schema and return it back to me\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "id": "dc2ba0be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9e3c6cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f49a53-57c3-4923-b44f-423b3d812bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"artist\": string  // The name of the musical artist\n",
            "\t\"song\": string  // The name of the song that the artist plays\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# The format instructions that LangChain makes. Let's look at them\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ],
      "id": "f9e3c6cf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d702900c"
      },
      "outputs": [],
      "source": [
        "# The prompt template that brings it all together\n",
        "# no need to be worried about the prompt engineering\n",
        "# Note: This is a different prompt template than before because we are using a Chat Model\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
        "                                                    {format_instructions}\\n{user_prompt}\")\n",
        "    ],\n",
        "    input_variables=[\"user_prompt\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "id": "d702900c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb6adde9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8268acd9-5e19-4dbf-b8ff-37078c77c170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a command from the user, extract the artist and song names \n",
            "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"artist\": string  // The name of the musical artist\n",
            "\t\"song\": string  // The name of the song that the artist plays\n",
            "}\n",
            "```\n",
            "Listening to Hurt from Johnny Cash is interesting\n"
          ]
        }
      ],
      "source": [
        "feeling_query = prompt.format_prompt(user_prompt=\"Listening to Hurt from Johnny Cash is interesting\")\n",
        "\n",
        "print (feeling_query.messages[0].content)"
      ],
      "id": "bb6adde9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8664302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b1ece7-a3be-413d-b0f9-4142c189aaf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'artist': 'Johnny Cash', 'song': 'Hurt'}\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "feeling_output = chat_model(feeling_query.to_messages())\n",
        "output = output_parser.parse(feeling_output.content)\n",
        "\n",
        "print (output)\n",
        "print (type(output))"
      ],
      "id": "b8664302"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b68b8eeb"
      },
      "source": [
        "Awesome, now we have a dictionary that we can use later down the line\n",
        "\n",
        "<span style=\"background:#fff5d6\">Warning:</span> The parser looks for an output from the LLM in a specific format. Your model may not output the same format every time. Make sure to handle errors with this one. GPT4 and future iterations will be more reliable.\n",
        "\n",
        "For more advanced parsing check out [Kor](https://eyurtsev.github.io/kor/)"
      ],
      "id": "b68b8eeb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Interacting with APIs\n",
        "\n",
        "*[LangChain API Interaction Docs](https://python.langchain.com/en/latest/use_cases/apis.html)*\n",
        "\n",
        "If the data or action you need is behind an API, you'll need your LLM to interact with APIs\n",
        "\n",
        "\n",
        "* **Use Cases:** Understand a request from a user and carry out an action, be able to automate more real-world workflows\n",
        "\n",
        "This topic is closely related to Agents and Plugins, though we'll look at a simple use case for this section. For more information, check out [LangChain + plugins](https://python.langchain.com/en/latest/use_cases/agents/custom_agent_with_plugin_retrieval_using_plugnplai.html) documentation.\n",
        "\n",
        "### Using DataForSEO\n",
        "In this example, I am using dataforseo.com\n",
        "DataForSEO aggregates data from search engines, marketplaces, review platforms, and billions of other websites across the web to provide you with unique insights for building innovative digital marketing solutions."
      ],
      "metadata": {
        "id": "UJR2mfesYonf"
      },
      "id": "UJR2mfesYonf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain's APIChain has the ability to read API documentation and understand which endpoint it needs to call.\n",
        "\n"
      ],
      "metadata": {
        "id": "x0Vav73eY0YG"
      },
      "id": "x0Vav73eY0YG"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install google-search-results\n",
        "\n",
        "import os\n",
        "from langchain.utilities.dataforseo_api_search import DataForSeoAPIWrapper\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"DATAFORSEO_LOGIN\"] = \"farnoushazour95@gmail.com\"\n",
        "os.environ[\"DATAFORSEO_PASSWORD\"] = \"0be13242d73ae956\"\n",
        "\n",
        "wrapper = DataForSeoAPIWrapper()\n",
        "\n",
        "wrapper.run(\"Weather in Gliwice?\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "6pKsH6jkZB8H",
        "outputId": "73881fc6-c57d-4cc5-f4e6-52c87d319372"
      },
      "id": "6pKsH6jkZB8H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10 Day Weather-Gliwice, Silesian Voivodeship, Poland. As of 9:55 am CEST. Today. 51°/47°. 80%. Thu 19 | Day. 51°. 80%. SE 7 mph.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.run(\"Weather in Montreal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PeXgCetldN9a",
        "outputId": "7ba64cda-bc69-4dbb-9747-790944b74c91"
      },
      "id": "PeXgCetldN9a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10 Day Weather-Montreal, Quebec, Canada. As of 8:59 pm EDT. Tonight. --/52°. 8%. Thu 19 | Night. 52°. 8%. SSE 6 mph. Cloudy. Low 52F.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.run(\"What does 1 United States Dollar equals to in Canadian Dollar \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YX5qe0G3z7YY",
        "outputId": "44964913-0b91-4e71-e802-bd361995f8a2"
      },
      "id": "YX5qe0G3z7YY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 USD = 1.371165 CAD Oct 19, 2023 01:21 UTC ... Check the currency rates against all the world currencies here. The currency converter below is easy to use and\\xa0...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Querying Tabular Data\n",
        "\n",
        "*[LangChain Querying Tabular Data Docs](https://python.langchain.com/en/latest/use_cases/tabular.html)*\n",
        "\n",
        "The most common type of data in the world sits in tabular form (ok, ok, besides unstructured data). It is super powerful to be able to query this data with LangChain and pass it through to an LLM\n",
        "\n",
        "* **Examples** - TBD\n",
        "* **Use Cases:** Use LLMs to query data about users, do data analysis, get real time information from your DBs\n",
        "\n",
        "One of the best projects\n",
        "For futher reading check out \"Agents + Tabular Data\" ([Pandas](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/pandas.html), [SQL](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html), [CSV](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/csv.html))\n",
        "\n",
        "Let's query an SQLite DB with natural language. We'll look at the [San Francisco Trees](https://data.sfgov.org/City-Infrastructure/Street-Tree-List/tkzw-k3nq) dataset."
      ],
      "metadata": {
        "id": "k5K8SIMSZY5z"
      },
      "id": "k5K8SIMSZY5z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![SQL](https://drive.google.com/uc?id=1RFf8e16Z_0LoWINhNmmIk2vFyho0t74N)\n"
      ],
      "metadata": {
        "id": "Zhv2yQQBZdqS"
      },
      "id": "Zhv2yQQBZdqS"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import SQLDatabase\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_experimental.sql import SQLDatabaseChain\n",
        "\n",
        "sqlite_db_path = \"/content/drive/MyDrive/ShareYourKnowledge-LangChain/San_Francisco_Trees.db\"\n",
        "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")\n",
        "\n",
        "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "3Mt3TwgRZo8J"
      },
      "id": "3Mt3TwgRZo8J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain.run(\"How many Species of trees are there in San Francisco?\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "EEewclmJZsRK",
        "outputId": "94bb42fc-6ab7-415d-a6db-d57ee9999e82"
      },
      "id": "EEewclmJZsRK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "How many Species of trees are there in San Francisco?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\";\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(578,)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mThere are 578 Species of trees in San Francisco.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 578 Species of trees in San Francisco.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is awesome! There are actually a few steps going on here.\n",
        "\n",
        "**Steps:**\n",
        "1. Find which table to use\n",
        "2. Find which column to use\n",
        "3. Construct the correct sql query\n",
        "4. Execute that query\n",
        "5. Get the result\n",
        "6. Return a natural language reponse back\n",
        "\n",
        "### Let's confirm via pandas"
      ],
      "metadata": {
        "id": "Ps0hOaRQZwBp"
      },
      "id": "Ps0hOaRQZwBp"
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to the SQLite database\n",
        "connection = sqlite3.connect(sqlite_db_path)\n",
        "\n",
        "# Define your SQL query\n",
        "query = \"SELECT count(distinct qSpecies) FROM SFTrees\"\n",
        "\n",
        "# Read the SQL query into a Pandas DataFrame\n",
        "df = pd.read_sql_query(query, connection)\n",
        "\n",
        "# Close the connection\n",
        "connection.close()"
      ],
      "metadata": {
        "id": "zaymgbeTZyuh"
      },
      "id": "zaymgbeTZyuh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result in the first column first cell\n",
        "print(df.iloc[0,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9zFVq6rZ0tq",
        "outputId": "15047b28-e099-4fc8-9d97-8e4f42fb24ca"
      },
      "id": "O9zFVq6rZ0tq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain for Deeper RCA\n",
        "* [OEM Parsing Package](https://github.com/b-yond-infinite-network/expl-oem-parsing/blob/main/src/expl_oem_parsing/extractor.py)\n",
        "* Using QAchain to [retrive](https://dbc-f54f5688-178a.cloud.databricks.com/?o=4514799458930737#job/1018187103737867/run/8036234537479) root error.\n"
      ],
      "metadata": {
        "id": "9wsgn7gTGfCD"
      },
      "id": "9wsgn7gTGfCD"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
