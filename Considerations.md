If you are thinking of implementing the Llama2 family or any other potential LLM, you may consider the following: 
* You can certainly try to implement Quantization to
   * Load the model on the lower GPU
   * Have your response faster
* Choice of model
  * Chat model
  * LLM model
  * Instruct model
    
* Prompt Engineering
  * Consider best practices
  * Zero-shot vs Few-shots
 
There are other families of open-source models that you can use for sure.
