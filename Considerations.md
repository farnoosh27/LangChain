If you are thinking of implementing Llama2 family or any other potential LLM, you may consider the following: 
* You can certainly try to implement Quantization to
   * Load the model on the lower GPU
   * Have your response faster
* Choice of model
  * Chat model
  * LLM model
  * Instruct model
* Prompt Engineering
  * Consider best practices
  * Zero-shot vs Few-shots
